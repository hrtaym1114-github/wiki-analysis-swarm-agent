from dotenv import load_dotenv
import os
import requests
from bs4 import BeautifulSoup
from swarm import Swarm, Agent
import urllib.parse
import time

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

class WikipediaAnalyzer:
    def __init__(self):
        self.base_url = "https://ja.wikipedia.org/wiki/"
        
    def collect_article_data(self, topic):
        """Wikipediaã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"\nğŸ” Wikipediaè¨˜äº‹ã€Œ{topic}ã€ã®å–å¾—ã‚’é–‹å§‹...")
        
        try:
            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡¦ç†å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            encoded_topic = urllib.parse.quote(topic)
            url = f"{self.base_url}{encoded_topic}"
            print(f"ğŸ“¡ ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹URL: {url}")
            
            # è¨˜äº‹ã®å–å¾—
            response = requests.get(url)
            print(f"ğŸ“¥ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
                title = soup.find(id="firstHeading").text
                print(f"ğŸ“‘ è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}")
                
                # ç›®æ¬¡ã®å–å¾—
                toc = soup.find(id="toc")
                sections = [link.text for link in toc.find_all('a')] if toc else []
                print(f"ğŸ“š ç›®æ¬¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(sections)}")
                
                # æœ¬æ–‡ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã®å–å¾—
                content = soup.find(id="mw-content-text")
                paragraphs = [p.text for p in content.find_all('p') if p.text.strip()]
                print(f"ğŸ“ å–å¾—ã—ãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•æ•°: {len(paragraphs)}")
                
                return {
                    'title': title,
                    'sections': sections,
                    'paragraphs': paragraphs
                }
            else:
                error_msg = f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {response.status_code}"
                print(f"âŒ {error_msg}")
                return {"error": error_msg}
                
        except Exception as e:
            error_msg = f"ä¾‹å¤–ãŒç™ºç”Ÿ: {str(e)}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg}

def create_research_agent():
    """èª¿æŸ»ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ"""
    print("\nğŸ¤– èª¿æŸ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
    return Agent(
        name="WikiResearcher",
        instructions="""
        ã‚ãªãŸã¯è³ªå•ã«åŸºã¥ã„ã¦Wikipediaè¨˜äº‹ã‚’æ¢ã™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
        
        ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å¿…ãšå®ˆã£ã¦ãã ã•ã„ï¼š
        1. å¿…ãšæ—¥æœ¬èªç‰ˆWikipediaã«å­˜åœ¨ã™ã‚‹åŸºæœ¬çš„ãªåè©ã®è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’é¸æŠ
        2. ä»¥ä¸‹ã®ã‚ˆã†ãªè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹ï¼š
           - å‹•è©ã‚„åŠ©è©ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹ï¼šã€Œã€œã«ãŠã‘ã‚‹ã€œã€ã€Œã€œã®æ­´å²ã€ï¼‰
           - é€ èªã‚„è¤‡åˆãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆä¾‹ï¼šã€ŒAIã®é€²åŒ–ã€ã€Œæœªæ¥æŠ€è¡“ã€ï¼‰
           - ã‚«ã‚¿ã‚«ãƒŠç”¨èªã¯åŸºæœ¬å½¢ã‚’ä½¿ç”¨ï¼ˆä¾‹ï¼šã€Œã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã€ã§ã¯ãªãã€Œè¨ˆç®—æ©Ÿã€ï¼‰
        
        3. è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªåŸºæœ¬çš„ãªåè©ã‚’é¸ã¶ï¼š
           - æŠ€è¡“ç”¨èªã®ä¾‹ï¼šäººå·¥çŸ¥èƒ½ã€æ©Ÿæ¢°å­¦ç¿’ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
           - æ¦‚å¿µã®ä¾‹ï¼šçŸ¥èƒ½ã€èªçŸ¥ã€å­¦ç¿’
           - åˆ†é‡ã®ä¾‹ï¼šæƒ…å ±å·¥å­¦ã€è¨ˆç®—æ©Ÿç§‘å­¦
        
        ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
        ---
        KEY1: [åŸºæœ¬åè©1]
        KEY2: [åŸºæœ¬åè©2]
        KEY3: [åŸºæœ¬åè©3]
        ---
        """
    )

def create_report_agent():
    """ãƒ¬ãƒãƒ¼ãƒˆä½œæˆç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ"""
    print("\nğŸ“ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
    return Agent(
        name="ReportWriter",
        instructions="""
        ã‚ãªãŸã¯åé›†ã•ã‚ŒãŸæƒ…å ±ã‚’æ•´ç†ã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
        
        ä»¥ä¸‹ã®å½¢å¼ã§å¿…ãšãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
        
        # åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
        
        ## ğŸ“‹ æ¦‚è¦
        [è³ªå•ã®è¦ç‚¹ã¨ä¸»è¦ãªç™ºè¦‹äº‹é …ã‚’ã¾ã¨ã‚ã‚‹]
        
        ## ğŸ” è©³ç´°åˆ†æ
        [é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®è©³ç´°ãªèª¬æ˜]
        
        ## ğŸ’¡ ã¾ã¨ã‚
        [è³ªå•ã«å¯¾ã™ã‚‹ç›´æ¥çš„ãªå›ç­”ã¨è¿½åŠ ã®è€ƒå¯Ÿ]
        
        ## ğŸ“š å‚è€ƒæƒ…å ±
        [ä½¿ç”¨ã—ãŸè¨˜äº‹ã‚„è¿½åŠ ã®å‚è€ƒæƒ…å ±]
        ---
        lang:ja
        """
    )

class WikipediaAnalyzer:
    def __init__(self):
        self.base_url = "https://ja.wikipedia.org/wiki/"
        
    def check_article_exists(self, topic):
        """Wikipediaã®è¨˜äº‹ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª"""
        encoded_topic = urllib.parse.quote(topic)
        url = f"{self.base_url}{encoded_topic}"
        response = requests.head(url)
        return response.status_code == 200

    def collect_article_data(self, topic):
        """Wikipediaã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"\nğŸ” Wikipediaè¨˜äº‹ã€Œ{topic}ã€ã®å–å¾—ã‚’é–‹å§‹...")
        
        if not self.check_article_exists(topic):
            print(f"âŒ è¨˜äº‹ã€Œ{topic}ã€ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return {"error": f"è¨˜äº‹ã€Œ{topic}ã€ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚"}
        
        try:
            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’å‡¦ç†å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            encoded_topic = urllib.parse.quote(topic)
            url = f"{self.base_url}{encoded_topic}"
            print(f"ğŸ“¡ ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹URL: {url}")
            
            # è¨˜äº‹ã®å–å¾—
            response = requests.get(url)
            print(f"ğŸ“¥ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®å–å¾—
                title = soup.find(id="firstHeading").text
                print(f"ğŸ“‘ è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}")
                
                # ç›®æ¬¡ã®å–å¾—
                toc = soup.find(id="toc")
                sections = [link.text for link in toc.find_all('a')] if toc else []
                print(f"ğŸ“š ç›®æ¬¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(sections)}")
                
                # æœ¬æ–‡ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã®å–å¾—
                content = soup.find(id="mw-content-text")
                paragraphs = [p.text for p in content.find_all('p') if p.text.strip()]
                print(f"ğŸ“ å–å¾—ã—ãŸãƒ‘ãƒ©ã‚°ãƒ©ãƒ•æ•°: {len(paragraphs)}")
                
                return {
                    'title': title,
                    'sections': sections,
                    'paragraphs': paragraphs
                }
            else:
                error_msg = f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—: {response.status_code}"
                print(f"âŒ {error_msg}")
                return {"error": error_msg}
                
        except Exception as e:
            error_msg = f"ä¾‹å¤–ãŒç™ºç”Ÿ: {str(e)}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg}

def run_analysis(query):
    """åˆ†æã®å®Ÿè¡Œ"""
    print(f"\nğŸ¯ åˆ†æã‚’é–‹å§‹: '{query}'")
    
    analyzer = WikipediaAnalyzer()
    client = Swarm()
    researcher = create_research_agent()
    report_writer = create_report_agent()
    
    # ç ”ç©¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    print("\nğŸ“š ç ”ç©¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’é–‹å§‹...")
    research_messages = [
        {
            "role": "user",
            "content": f"ä»¥ä¸‹ã®è³ªå•ã«é–¢é€£ã™ã‚‹Wikipediaã®è¨˜äº‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š\n{query}"
        }
    ]
    
    research_response = client.run(
        agent=researcher,
        messages=research_messages
    )
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡ºã¨æ¤œè¨¼
    keywords = []
    research_result = research_response.messages[-1]["content"]
    for line in research_result.split('\n'):
        if line.startswith('KEY'):
            keyword = line.split(':')[1].strip()
            # è¨˜äº‹ã®å­˜åœ¨ç¢ºèªã‚’è¡Œã£ã¦ã‹ã‚‰è¿½åŠ 
            if analyzer.check_article_exists(keyword):
                keywords.append(keyword)
            else:
                print(f"âš ï¸ è­¦å‘Š: è¨˜äº‹ã€Œ{keyword}ã€ã¯å­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                # è¨˜äº‹ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ç ”ç©¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å†åº¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                if len(keywords) < 2:  # æœ€ä½2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¢ºä¿
                    retry_messages = [
                        {
                            "role": "user",
                            "content": f"å‰å›ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®åŸºæœ¬çš„ãªè¨˜äº‹åã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
                        }
                    ]
                    retry_response = client.run(
                        agent=researcher,
                        messages=retry_messages
                    )
                    new_keyword = retry_response.messages[-1]["content"].split(':')[-1].strip()
                    if analyzer.check_article_exists(new_keyword):
                        keywords.append(new_keyword)
    
    print(f"\nğŸ”‘ æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
    
    # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã¤ã„ã¦Wikipediaè¨˜äº‹ã‚’å–å¾—
    all_data = []
    for keyword in keywords:
        print(f"\nğŸ“– ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã®è¨˜äº‹ã‚’å–å¾—ä¸­...")
        data = analyzer.collect_article_data(keyword)
        if "error" not in data:
            all_data.append(data)
    
    if not all_data:
        return "é–¢é€£ã™ã‚‹è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    print("\nâœï¸ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚’é–‹å§‹...")
    report_messages = [
        {
            "role": "user",
            "content": f"""
            ä»¥ä¸‹ã®è³ªå•ã¨åé›†ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
            
            è³ªå•ï¼š{query}
            
            åé›†ãƒ‡ãƒ¼ã‚¿ï¼š
            {str(all_data)}
            """
        }
    ]
    
    report_response = client.run(
        agent=report_writer,
        messages=report_messages
    )
    
    return report_response.messages[-1]["content"]
if __name__ == "__main__":
    print("ğŸ¤– Wikipediaåˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•ã—ã¾ã—ãŸ")
    print("----------------------------------------")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹
    user_query = input("ğŸ” èª¿ã¹ãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š")
    
    # å…¨ä½“ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
    total_start_time = time.time()
    
    result = run_analysis(user_query)
    
    print("\nğŸ“Š æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ:")
    print("----------------------------------------")
    print(result)
    print("----------------------------------------")
    print(f"\nâ±ï¸ ç·å‡¦ç†æ™‚é–“: {time.time() - total_start_time:.2f}ç§’")